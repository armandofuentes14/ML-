{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INDICE PISOS RM\n",
    "\n",
    "### 1. ESTRUCTURA DE LAS CARPETAS Y ARCHIVOS \n",
    "    \n",
    "    1.1 Bases de datos\n",
    "    1.2 Carpetas:  Modelo_data_sucios y Modelo_sin_feature_inge\n",
    "    1.3 Orden de revision de los Notebook principales\n",
    "  \n",
    "### 2. EXPLICACIÓN DEL CÓDIGO DE LOS NOTEBOOK PRINCIPALES\n",
    "    2.1 pisos_rm_clean\n",
    "    2.2 pisos_rm_analisis\n",
    "    2.3 pisos_rm_mode_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. ESTRUCTURA DE LAS CARPETAS Y ARCHIVOS "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bases de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las bases de datos necesarias para trabajar en cada uno de los notebook los encontraras en la carpeta data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Carpetas:  Modelo_data_sucios y Modelo_sin_feature_inge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estas carpetas encontraras los modelos iniciales, y como bien indican sus nombres hacen alusión al proceso de aprendizaje y de la evolucion de los resultados. \n",
    "\n",
    "Dicho esto, no estan los resultados finales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Orden de revision de los Notebook principales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1º pisos_rm_clean: En este notebook se realiza la primera exploración de los datos, la identificacion e eliminacion de errores de escritura y outlier.\n",
    "\n",
    "\n",
    "\n",
    "2º pisos_rm_analisis: En este notebook analizaremos la distribucion de los datos de nuestras variables y del target, la correlación existente y posteriormente la union con las variables anexadas.   \n",
    "\n",
    "\n",
    "\n",
    "3º pisos_modelo1: Habiendo realizado la limpieza de datos y feature inge. encontraremos en este notebook cada uno de los modelos probados, sus resultados respectivos, posteriormente identificaremos los dos mejores modelos, a los cuales aplicaremos pipeline y Gridsearch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EXPLICACIÓN DEL CÓDIGO DE LOS NOTEBOOK PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1 pisos_rm_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportamos la librerias correpondientes.\n",
    "\n",
    "1) Extraemos los datos:\n",
    "\n",
    "    df= pd.read_excel(\"data/pisos_rm.xlsx\n",
    "\n",
    "\n",
    "2) Identificamos NaN, naturaleza de los valores 0 y tipo de datos por feature\n",
    "\n",
    "    status(df).sort_values(by=\"q_nan\",ascending=False)\n",
    "\n",
    "3) Identificamos los valores máximos y mínimos de nuestro target\n",
    "\n",
    "    df.max()\n",
    "    df.min()\n",
    "\n",
    "4) Revisamos si existen datos duplicados \n",
    "\n",
    "     df.duplicated().sum()  \n",
    "\n",
    "5) Convertimos los valores de nuestro target en INT\n",
    "\n",
    "    df[\"UF\"]=df[\"UF\"].astype(int)\n",
    "\n",
    "6) Visualizamos la correlación de nuestras variables \n",
    "\n",
    "    sns.heatmap(df.corr(), annot=True)\n",
    "\n",
    "7) Visualizamos la distribucion de los datos  \n",
    "    \n",
    "    sns.pairplot(df)\n",
    "\n",
    "8) Normalizamos los valores de \"UF\", creando una nueva columna\n",
    "\n",
    "    df[\"Uf_log\"]=np.log1p(df[\"UF\"])#luego al precio en \"UF_log\"\n",
    "    \n",
    "    Poeteriormente habrá que aplicar la funcion matematica inversa, back = np.expm1()\n",
    "\n",
    "9) Separamos la columna dirección, creando la columna comuna\n",
    "\n",
    "    df[\"Direccion\"].str.split(\",\",expand= True)\n",
    "    direc=df[\"Direccion\"].str.split(\",\",expand= True)\n",
    "\n",
    "    direc.drop(direc.columns[2:12],axis=1, inplace= True)\n",
    "\n",
    "    direc= direc.rename(columns={0:\"Comuna\", 1:\"direcc\"})\n",
    "\n",
    "10) Las unimos a nuestro dataframe \n",
    "\n",
    "    df=pd.concat([df,direc],axis=1)\n",
    "\n",
    "11) Eliminamos la columna Direccion \n",
    "\n",
    "    df.drop([\"Direccion\"],axis=1)\n",
    "\n",
    "ELIMINAMOS LOS ERRORES DE ESCRITURA Y OUTLIER\n",
    "\n",
    "12) Eliminamos la columna m2 construidos (porque tiene los mismo valores que m2 totales )\n",
    "\n",
    "    df.drop([\"m2_Construidos\"],axis=1)\n",
    "\n",
    "13) ELiminamos todos los pisos con m2 totales igual a 0 \n",
    "\n",
    "    df.loc[df[\"m2_totales\"]>0]\n",
    "    df=df.loc[df[\"m2_totales\"]>0]\n",
    "\n",
    "14) Eliminamos todos los pisos que valen menos de 20 millones de pesos\n",
    "\n",
    "    df=df.loc[df[\"Precio\"]>20000000]\n",
    "\n",
    "15) Eliminamos todos los pisos que valen mas 15 * 10^8   \n",
    "\n",
    "    df=df.loc[df[\"Precio\"]<1500000000]\n",
    "\n",
    "16) Eliminamos todos los pisos que tengan menos de 20 m2 \n",
    "\n",
    "    df=df.loc[df[\"m2_totales\"]>19]\n",
    "\n",
    "17) Eliminamos todos los pisos que tengan mas de 5 estacionamientos, 5 baños y 6 habitacines\n",
    "\n",
    "    df=df.loc[df[\"Estacionamientos\"]<=5]\n",
    "    df=df.loc[df[\"Banos\"]<6]\n",
    "    df=df.loc[df[\"Habitaciones\"]<7]\n",
    "\n",
    " 18) Vemos la nueva Distribucion    \n",
    "    \n",
    "    sns.pairplot(df)\n",
    "\n",
    "19) guardamos nuestra nueva base de datos con todos los        cambios realizados\n",
    "\n",
    "    df.to_excel(\"data/clean_pisos.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 pisos_rm_analisis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportamos las librerias correspondientes\n",
    "\n",
    "1) cargamos nuestra base de datos \n",
    "\n",
    "    piso=pd.read_excel(\"data/clean_pisos.xlsx\")\n",
    "\n",
    "2) le cambiamos el nombre a nuestro target \n",
    "\n",
    "    piso.rename(columns={\"Uf_log\":\"UF_log\"},inplace=True)\n",
    "\n",
    "3) Realizamos un análisis exploratorio de los datos depurados\n",
    "\n",
    "    status(piso).sort_values(by=\"q_nan\", ascending=False)\n",
    "    piso.describe().T.round(0)\n",
    "    piso.max()\n",
    "    pisos.min()\n",
    "\n",
    "4) Observamos la distribución de los datos con Histogramas\n",
    "\n",
    "    def plot_hist(variable):\n",
    "    plt.figure(figsize = (9,3))\n",
    "    plt.hist(piso[variable], bins = 50 , rwidth=.85)\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"{} Distribución de los Datos con Histograma\".format(variable))\n",
    "    plt.show()\n",
    "\n",
    "    numero_variable =['Precio', 'UF','UF_log', 'Habitaciones', 'Banos',\n",
    "       'Estacionamientos']\n",
    "\n",
    "    for n in numero_variable:\n",
    "        plot_hist(n)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    grf=piso[\"Comuna\"].value_counts().plot(ax=ax, kind='bar',figsize = (12,6))    \n",
    "\n",
    "5)  Vemos la nueva correlación de los datos \n",
    "\n",
    "    sns.heatmap(piso.corr(), annot=True)\n",
    "\n",
    "6) Cargamos y revisamos las nuevas variables\n",
    "\n",
    "    variables=pd.read_excel(\"data/numero_pisos.xlsx\") \n",
    "    status(variables).sort_values(by=\"q_nan\",ascending=False)    \n",
    "\n",
    "7) Cambiamos el orden de la columnas para favorecer el cruce de datos con el dataframe variables  \n",
    "\n",
    "    piso = piso[['Comuna','m2_totales', 'Habitaciones', 'Banos',\"Estacionamientos\", 'Publi','Precio', 'UF',\"UF_log\"]] \n",
    "\n",
    "8) Pasamos la columna comuna a indice para facilitar la union con el dataframe principal pisos \n",
    "\n",
    "\n",
    "    variables.set_index(\"Comuna\",inplace=True)\n",
    "\n",
    "9) Unimos los dos datagframe \n",
    "\n",
    "    union= piso.join(variables, on=[\"Comuna\"], how= \"inner\")\n",
    "\n",
    "10) revisamos la nueva correlacion con el target \n",
    "\n",
    "    union.corr()\n",
    "    correlacion= union.corr()\n",
    "    correlacion[\"UF\"].sort_values(ascending=False)\n",
    "\n",
    "11) visualizamos la distribución de las nuevas variables y la correlación con el target por categorias\n",
    "\n",
    "Otros \n",
    "\n",
    "cols_to_plot1= ['UF_log',\n",
    "       'Poblacion total 2020', 'Superficie de Área Verde m²',\n",
    "       'Superficie Plazas m²',' luminarias cada 50 metros lineales ',\n",
    "       ' Consumo Per Capita residencial (Kwh/persona)',\n",
    "       'Fallos suministro electrico',]\n",
    "\n",
    "       sns.heatmap(union[cols_to_plot1].corr(), annot=True)\n",
    "\n",
    "Educación:\n",
    "\n",
    "        cols_to_plot2= ['UF_log',\n",
    "      \n",
    "       'Distancia a Educación Inicial (m)',\n",
    "       'Razón entre disponibilidad efectiva de matrículas y demanda potencial por educación básica',\n",
    "       'Número de establecimientos eduacion basica',\n",
    "       'Número de establecimientos privados',\n",
    "       'Distancia a Educación Basica (m)',\n",
    "      ]\n",
    "\n",
    "      sns.pairplot(union[cols_to_plot2])\n",
    "\n",
    "Salud:\n",
    "        cols_to_plot3=[\"UF_log\",'Número de establecimientos atencion primaria',\n",
    "       'Distancia a Centro de Salud (m)','% fonasa (Prevision Salud)',\n",
    "       '% ff.aa. y del orden (Prevision Salud)', '% isapre (Prevision Salud)',\n",
    "       ' % ninguno (Prevision Salud)', '% otro sistema (Prevision Salud)',]\n",
    "\n",
    "       sns.heatmap(union[cols_to_plot3].corr(), annot=True)\n",
    "\n",
    "Infraestructura y servicios:\n",
    "\n",
    "       cols_to_plot4=[\"UF_log\",'porcentaje manzanas con veredas con buena calidad de pavimento',\n",
    "       'Total de residuos (t)',\n",
    "       ' Porcentaje de residuos municipales valorizados',\n",
    "       ' Porcentaje de viviendas en mal estado  y/o carente  servicios básicos',\n",
    "       'Porcentaje de participación del FCM en el Ingreso Municipal Total',\n",
    "       'Tasa de conexiones residenciales fijas de internet por cada 1.000 viviendas particulares',\n",
    "       'Longitud ciclovías (km)', 'Tiempo de viaje en trans publico hr punta','% de Acceso deficitario a Servicios Basicos ',]\n",
    "\n",
    "\n",
    "        sns.heatmap(union[cols_to_plot4].corr(), annot=True)\n",
    "\n",
    " Pobreza y vulnerabilidad Social:\n",
    "\n",
    "        cols_to_plot5 = [\"UF_log\",'Víctimas Robo con violencia o Intimidación',\n",
    "       'Víctimas Robo por Sorpresa',\n",
    "       ' Número de denuncias por delito en el espacio público cada 100 habitantes',\n",
    "       ' Porcentaje de unidades vecinales que tienen entre 20% y 60% de hogares vulnerables',\n",
    "       ' Porcentaje de la población en situación de pobreza (pobreza multidimensional MDSF)',\n",
    "       'Porcentaje de Hacinamiento',\n",
    "       'Porcentaje de viviendas con situación de allegamiento externo',\n",
    "       ' Requerimiento de viviendas nuevas urbanas','Ingreso promedio del hogar por Comuna',] \n",
    "\n",
    "       sns.heatmap(union[cols_to_plot5].corr(), annot=True)\n",
    "\n",
    "12) Transformamos cada comuna en una variable y las unimos al  dataframe principal\n",
    "\n",
    "        dummis_comunas= pd.get_dummies(union[\"Comuna\"])\n",
    "        dummis_comunas.head()\n",
    "        pisos=pd.concat([union,dummis_comunas],axis=1)\n",
    "\n",
    "13) cambiamos el uint8 a int de cada una de las nuevas columnas, que son las comunas  \n",
    "\n",
    "        le=LabelEncoder()\n",
    "\n",
    "\n",
    "    for i in pisos.select_dtypes(\"uint8\").keys(): #\n",
    "        pisos[i]=le.fit_transform(pisos[i])\n",
    "\n",
    "14) Guardamos nuestra nueva base de datos \n",
    "\n",
    "    pisos.to_excel(\"data/pisos_analis_data.xlsx\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 pisos_rm_mode_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportamos las librerias correspondientes\n",
    "\n",
    "1) Extraemos los datos:\n",
    "\n",
    "    pisos=pd.read_excel(\"data/pisos_analis_data.xlsx\")\n",
    "\n",
    "2) definimos las variables \n",
    "\n",
    "    X= pisos.drop([\"Comuna\",'Publi','Precio', 'UF', \"UF_log\",'Porcentaje de viviendas con situación de allegamiento externo','Víctimas Robo por Sorpresa', 'Número de establecimientos educacion inicial','Distancia a Educación Basica (m)'], axis=1)\n",
    "    y= pisos[\"UF_log\"]\n",
    "\n",
    "    seed=12\n",
    "\n",
    "3) Dividimos los datos en test y train \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = seed)\n",
    "\n",
    "    print(\"Total features shape:\", X.shape)\n",
    "    print(\"Train features shape:\", X_train.shape)\n",
    "    print(\"Train target shape:\", y_train.shape)\n",
    "    print(\"Test features shape:\", X_test.shape)\n",
    "    print(\"Test target shape:\", y_test.shape)\n",
    "\n",
    "4) Aplicamos modelos \n",
    "\n",
    "\n",
    "5) Aplicamos  las métricas a los dos mejores modelos y visualizamos el feature importance  \n",
    "\n",
    "2º mejor modelo: from xgboost import XGBRegressor\n",
    "\n",
    "    xgb_reg = XGBRegressor(n_estimators=50, random_state=seed)\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Score train :\", metrics.r2_score(y_train, xgb_reg.predict(X_train)))\n",
    "    print(\"Score test:\", metrics.r2_score(y_test, xgb_reg.predict(X_test)))\n",
    "    acc5 = metrics.r2_score(y_test, xgb_reg.predict(X_test))\n",
    "\n",
    "    predictions = xgb_reg.predict(X_test)\n",
    "\n",
    "    print('MAE:', metrics.mean_absolute_error(np.expm1(y_test), np.expm1(predictions)))  \n",
    "    print('MSE:', metrics.mean_squared_error(np.expm1(y_test), np.expm1(predictions)))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(predictions))))\n",
    "    print('MAPE:', metrics.mean_absolute_percentage_error(np.expm1(y_test), np.expm1(predictions)))\n",
    "\n",
    "    xgb_reg.feature_importances_\n",
    "    df_impor_xgb=pd.DataFrame({\"col\":X_train.columns, \"coef_impor\":xgb_reg.feature_importances_})\n",
    "    df_impor_xgb = df_impor_xgb.round(decimals=2)\n",
    "    df_impor_xgb.sort_values(by= \"coef_impor\", ascending = False).head(10)\n",
    "\n",
    "\n",
    "\n",
    "Mejor modelo: from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "    gbrt = GradientBoostingRegressor(n_estimators=300, random_state=seed,max_depth=3)\n",
    "    gbrt.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Score train :\", metrics.r2_score(y_train, gbrt.predict(X_train)))\n",
    "    print(\"Score test:\", metrics.r2_score(y_test, gbrt.predict(X_test)))\n",
    "    print('MAE:', metrics.mean_absolute_error(np.expm1(y_test), np.expm1(predictions)))  \n",
    "    print('MSE:', metrics.mean_squared_error(np.expm1(y_test), np.expm1(predictions)))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(predictions))))\n",
    "    print('MAPE:', metrics.mean_absolute_percentage_error(np.expm1(y_test), np.expm1(predictions)))\n",
    "\n",
    "    gbrt.feature_importances_\n",
    "    df_import_gbrt= pd.DataFrame({\"col\":X_train.columns, \"coef_impor\": gbrt.feature_importances_})\n",
    "    df_import_gbrt= df_import_gbrt.round(decimals=2)\n",
    "    df_import_gbrt.sort_values(by= \"coef_impor\",ascending= False).head(10)\n",
    "\n",
    "\n",
    "\n",
    "6) Aplicamos PCA & Pipeline a los dos mejores modelos\n",
    "\n",
    "    pipeline = Pipeline([('scaler',StandardScaler()),(\"pca\",PCA(n_components=4)), ('regressor', GradientBoostingRegressor())])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    r2_train=pipeline.score(X_train, y_train)\n",
    "    print(f\"GBR_train: {r2_train}\")\n",
    "\n",
    "    r2 = pipeline.score(X_test, y_test)\n",
    "    print(f\"GBR_test: {r2}\")\n",
    "\n",
    " 7) Aplicamos pipeline y GridsearchCV a los dos mejores modelos  \n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"scaler\", StandardScaler()),# este metodo sirve para escalar los datos \n",
    "        ('regressor', LinearRegression()) \n",
    "    ])\n",
    "\n",
    "\n",
    "    bosting_ressor_params = {\n",
    "        'regressor': [GradientBoostingRegressor()],\n",
    "        'regressor__max_depth': [3,4,5,6,7,8],\n",
    "        'regressor__n_estimators': [220,240,260,280]\n",
    "    }\n",
    "\n",
    "\n",
    "    search_space = [\n",
    "    \n",
    "        bosting_ressor_params,\n",
    "    ]\n",
    "\n",
    "    clf_b = GridSearchCV(estimator = pipe,\n",
    "                    param_grid = search_space,\n",
    "                    cv = 3)\n",
    "\n",
    "    clf_b.fit(X_train, y_train)  \n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(clf_b.best_estimator_)\n",
    "    print(clf_b.best_score_) # 0,76\n",
    "    print(clf_b.best_params_)\n",
    "    \n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",grid_XGB.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",grid_XGB.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",grid_XGB.best_params_)\n",
    "\n",
    "8) GridSearchCV sin pipeline  al mejor modelo\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = seed)\n",
    "\n",
    "\n",
    "    GBR = GradientBoostingRegressor()\n",
    "    parameters = {'learning_rate': [0.01,0.02,0.03,0.04], #tasa_de_aprendizaje\n",
    "                  'subsample'    : [0.9, 0.5, 0.2, 0.1],#submuestra\n",
    "                  'n_estimators' : [300,350,400],\n",
    "                  'max_depth'    : [3,4,5,6,8,]# profundidad\n",
    "                 }\n",
    "\n",
    "    grid_GBR = GridSearchCV(estimator=GBR,  \n",
    "                        param_grid = parameters,  # aqui van los parametros de los modelos utilizados\n",
    "                        cv = 3,    #  cross validation \n",
    "                        n_jobs=-1) # Esto significa la cantidad de trabajos que se ejecutarán en paralelo, -1 significa usar todo el procesador.\n",
    "\n",
    "    grid_GBR.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "    print(\" Results from Grid Search \" )\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",grid_GBR.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",grid_GBR.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",grid_GBR.best_params_)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa388cf9dcdc3f59653930fe6daadf8dc172d64b4b87278f30ee7891a45c6ac3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
